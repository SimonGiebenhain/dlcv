\documentclass{article}
\usepackage[en]{ukon-infie}
\usepackage[utf8]{inputenc}
\usepackage{algorithm2e}
\usepackage{amsmath}
\usepackage{graphicx}
% kann de oder en sein
% kann bubble break, topexercise sein

\Names{Jonas Probst, Simon Giebenhain, Gabriel Scheibler, Clemens Gutknecht}
\Lecture[DLCV]{Deep Learniing for Computer Vision}
\Term{WS 2017/18}

\begin{document}
    \begin{ukon-infie}[12.11.17]{1}

		
		\begin{exercise}[p=20]{}
        	
		\end{exercise}
		
		\begin{exercise}[p=10]{}
        	
		\end{exercise}
		
		\begin{exercise}[p=10]{}
        	
		\end{exercise}
		
		\begin{exercise}[p=10]{}
        	
		\end{exercise}
		
		\begin{exercise}[p=10]{}
        	
		\end{exercise}
		
		\begin{exercise}[p=10]{}
        	Let $E: \mathbb{R}^3 \rightarrow \mathbb{R}, E(\Theta) = 2\Theta_1^2 + 4\Theta_2 + \text{max}(0, \Theta_2 + \Theta_3)$ be a loss function. In order to perfrom the gradient descent, we first give the gradient of $E$ (which only makes sense if $\Theta_2 + \Theta_3 \not = 0$ ): 
\begin{equation}
\nabla E(\Theta) =
\begin{cases}
\left(
\begin{array}{c}
4 \Theta_1\\
5\\
1\\
\end{array}
\right) & \text{für } \Theta_2 + \Theta_3 > 0 \\
 \left(
\begin{array}{c}
4\Theta_1\\
1\\
0\\
\end{array}
\right) & \text{für } \Theta_2 + \Theta_3 < 0\\
\end{cases}
\end{equation}

With this we compute the following:\\

$\nabla E(\Theta^{[0]}) = 
\left( \begin{array}{c}
8\\
5\\
1\\
\end{array}\right) \Rightarrow \Theta^{[1]} = 
\left( \begin{array}{c}
2\\
1\\
0\\
\end{array} \right)
- \frac{1}{2}
\left( \begin{array}{c}
8\\
5\\
1\\
\end{array} \right) = 
\left( \begin{array}{c}
-2\\
-1.5\\
-0.5\\
\end{array} \right)
$ and\\ 
$\nabla E(\Theta^{[1]}) = 
\left( \begin{array}{c}
-8\\
4\\
0\\
\end{array}\right) \Rightarrow \Theta^{[2]} = 
\left( \begin{array}{c}
-2\\
-1.5\\
-0.5\\
\end{array} \right)
- \frac{1}{2}
\left( \begin{array}{c}
-8\\
4\\
0\\
\end{array} \right) = 
\left( \begin{array}{c}
2\\
-3.5\\
-0.5\\
\end{array} \right)$


		\end{exercise}
		
		\begin{exercise}[p=10]{}
		Let $f: \mathbb{R}^n \rightarrow \mathbb{R}$ and $g: \mathbb{R} \rightarrow \mathbb{R}$ be two differentiable functions. Consider the concatenation $h = g \circ f$. Let $p \in \mathbb{R}^n$ and let $r_f , r_g$ denote the remainders of the linear approximation of $f$ and $g$. Since both $f$ and $g$ are differentiable it holds, that $\lim_{\tau \to 0}\frac{r_f(\tau)}{\tau} = 0$ and $\lim_{\tau \to 0}\frac{r_g(\tau)}{\tau} = 0$ \\
		\begin{eqnarray}
		g(f(p + \tau h)) - g(f(p)) & = & g'(f(p))(f(p + \tau h) - f(p)) + r_g(f(p + \tau h) - f(p)) \\
												& = & g'(f(p))(\tau \nabla f(p)^T h + r_f(\tau)) + r_g(f(p + \tau h) - f(p)) \\
											  & = & g'(f(p))\tau \nabla f(p)^T h + g'(f(p)) r_f(\tau)) + r_g(f(p + \tau h) - f(p))
		\end{eqnarray}
		Now we have still have to show that $lim_{\tau \to 0}g'(f(p)) r_f(\tau)) + r_g(f(p + \tau h) - f(p)) = 0$, this holds because:\\
		$$\lim_{\tau \to 0}\frac{r_f(\tau)}{\tau} = 0$$ and $$\lim_{f(p + \tau h) - f(p) \to 0}\frac{r_g(f(p + \tau h) - f(p))}{f(p + \tau h) - f(p)} = \lim_{\tau \to 0}\frac{r_g(\tau)}{\tau} = 0$$With this we can rewrite (3) to: \\
		
		$$h (p + \tau h) - h(p) = g(f(p + \tau h)) - g(f(p)) = g'(f(p))\tau \nabla f(p)^T h + r_h(\tau)$$ with $\lim_{\tau \to 0}\frac{r_h(\tau)}{\tau} = 0$. \\This matches exactly to the definiton(as a linear approximation) of the gradient/derivative.
        	
		\end{exercise}
		
		


		\begin{exercise}[p=2]{Data Mining and Visualization}
		
		\question{}{
			The subway map or bus map is an example of visulazation in my everyday life. It conveys the most important information very well, while ignoring details. Thus the visualization is very easy and quick to understand.
		}
		
		\question{}{
			The Google advertisments are the most prominant examples of data mining in my everyday life. Data is mined from my browser history, in oder to display fitting ads.
		}

		\end{exercise}

		\begin{exercise}[p=4]{Visualization: Human vs. Computer}
		\question{}{
			It makes sense, because it can be very tedious to visualize huge data sets by hand. This can be solved well by automation. \\
			Moreover digital visualzations can be updated (e.g. improve visualization, incorporate new data) at every time without much effort, whereas non-digital visualizations have to be drawn from scratch.
		}
		\question{}{
		The raw data extracted by the computer is hard to interpret. \\
		The visulaization makes it much easier to interpret the result.\\
		Additionaly this can lead to new insights/discovery of pattern and inspire a change in the algorithm applied proviously.
		}
		\end{exercise}
		
		
\end{ukon-infie}
\end{document}
